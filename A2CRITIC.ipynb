{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOkIBkszJ8jrBM2hz5dMvAN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Loki-33/RL-Algos/blob/main/A2CRITIC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4afq1nw1t-T"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import gym\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('CartPole-v1', render_mode='rgb_array')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgdfCIqk7HTb",
        "outputId": "40dead21-ef49-49b2-f560-1c00bdef96af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_obs = env.observation_space.shape[0]\n",
        "n_actions = env.action_space.n"
      ],
      "metadata": {
        "id": "M_J0_2xG7HNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gamma = 0.99\n",
        "lr = 1e-3\n",
        "entropy_coef = 0.01\n",
        "value_coef = 0.5\n",
        "episodes = 1000"
      ],
      "metadata": {
        "id": "R453L3BZ7HDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class A2C(nn.Module):\n",
        "  def __init__(self, n_obs, n_acts):\n",
        "    super().__init__()\n",
        "\n",
        "    self.net = nn.Sequential(\n",
        "      nn.Linear(n_obs, 128),\n",
        "      nn.ReLU(),\n",
        "    )\n",
        "\n",
        "    self.actor = nn.Linear(128, n_acts)\n",
        "    self.critic = nn.Linear(128, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.net(x)\n",
        "    return self.actor(x), self.critic(x).squeeze(-1)"
      ],
      "metadata": {
        "id": "XK8RP-ua7Gs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = A2C(n_obs, n_actions)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "0wwN163c7o3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_returns(rewards, dones, next_value, gamma=0.99):\n",
        "  returns = np.zeros(len(rewards))\n",
        "  R = next_value\n",
        "  for i in reversed(range(len(rewards))):\n",
        "    R = rewards[i] + gamma * R * (1 - dones[i])\n",
        "    returns[i] = R\n",
        "  return returns"
      ],
      "metadata": {
        "id": "JhF26ItY77p9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_data = []\n",
        "episode_reward=[]\n",
        "\n",
        "for ep in range(episodes):\n",
        "  state = env.reset()\n",
        "  done = False\n",
        "  rewards = []\n",
        "  dones = []\n",
        "  values = []\n",
        "  log_probs = []\n",
        "  entropies = []\n",
        "\n",
        "  while not done:\n",
        "    state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "    logits, value = model(state_tensor)\n",
        "    probs = torch.softmax(logits, dim=-1)\n",
        "    dist = torch.distributions.Categorical(probs)\n",
        "    action = dist.sample()\n",
        "    log_prob = dist.log_prob(action)\n",
        "    entropy = dist.entropy()\n",
        "    next_state, reward, done, truncated= env.step(action.item())\n",
        "\n",
        "    dones.append(bool(done) or bool(truncated))\n",
        "    values.append(value)\n",
        "    rewards.append(reward)\n",
        "    log_probs.append(log_prob)\n",
        "    entropies.append(entropy)\n",
        "\n",
        "    state = next_state\n",
        "\n",
        "  state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "  _, next_value = model(state_tensor)\n",
        "  returns = compute_returns(rewards, dones, next_value)\n",
        "\n",
        "  returns = torch.tensor(returns)\n",
        "  values = torch.cat(values).squeeze()\n",
        "  log_probs = torch.stack(log_probs)\n",
        "  entropies = torch.stack(entropies)\n",
        "\n",
        "  advantage = returns - values.detach()\n",
        "  advantage = (advantage-advantage.mean()) / (advantage.std() + 1e-8)\n",
        "  policy_loss = -(log_probs * advantage).mean()\n",
        "  value_loss = value_coef * (returns - values).pow(2).mean()\n",
        "  entropy_loss = entropy_coef * entropies.mean()\n",
        "\n",
        "  loss = policy_loss + value_loss + entropy_loss\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "  optimizer.step()\n",
        "\n",
        "  if ep % 10 == 0:\n",
        "    print(f\"Episode {ep}, Return: {sum(rewards):.2f}, Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeGGXJRy8e5j",
        "outputId": "ade4cf4e-3d59-4983-bf93-15ece1a66197"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0, Return: 10.00, Loss: 21.9274\n",
            "Episode 10, Return: 56.00, Loss: 373.8926\n",
            "Episode 20, Return: 16.00, Loss: 44.4280\n",
            "Episode 30, Return: 16.00, Loss: 42.5700\n",
            "Episode 40, Return: 13.00, Loss: 28.0122\n",
            "Episode 50, Return: 15.00, Loss: 34.5982\n",
            "Episode 60, Return: 10.00, Loss: 14.2502\n",
            "Episode 70, Return: 30.00, Loss: 120.1060\n",
            "Episode 80, Return: 12.00, Loss: 18.3501\n",
            "Episode 90, Return: 26.00, Loss: 86.7701\n",
            "Episode 100, Return: 12.00, Loss: 16.4571\n",
            "Episode 110, Return: 19.00, Loss: 42.2440\n",
            "Episode 120, Return: 24.00, Loss: 64.6781\n",
            "Episode 130, Return: 84.00, Loss: 596.1627\n",
            "Episode 140, Return: 16.00, Loss: 23.9809\n",
            "Episode 150, Return: 43.00, Loss: 180.6599\n",
            "Episode 160, Return: 12.00, Loss: 9.4395\n",
            "Episode 170, Return: 36.00, Loss: 126.1465\n",
            "Episode 180, Return: 49.00, Loss: 215.8163\n",
            "Episode 190, Return: 57.00, Loss: 273.1129\n",
            "Episode 200, Return: 50.00, Loss: 208.8251\n",
            "Episode 210, Return: 27.00, Loss: 51.9026\n",
            "Episode 220, Return: 60.00, Loss: 273.4620\n",
            "Episode 230, Return: 23.00, Loss: 24.2339\n",
            "Episode 240, Return: 32.00, Loss: 59.3928\n",
            "Episode 250, Return: 68.00, Loss: 299.2191\n",
            "Episode 260, Return: 50.00, Loss: 157.8060\n",
            "Episode 270, Return: 49.00, Loss: 140.8979\n",
            "Episode 280, Return: 60.00, Loss: 206.6444\n",
            "Episode 290, Return: 39.00, Loss: 72.1273\n",
            "Episode 300, Return: 40.00, Loss: 70.7243\n",
            "Episode 310, Return: 58.00, Loss: 159.9230\n",
            "Episode 320, Return: 72.00, Loss: 242.2245\n",
            "Episode 330, Return: 22.00, Loss: 19.6792\n",
            "Episode 340, Return: 31.00, Loss: 31.8844\n",
            "Episode 350, Return: 54.00, Loss: 106.2050\n",
            "Episode 360, Return: 37.00, Loss: 40.5375\n",
            "Episode 370, Return: 45.00, Loss: 60.6845\n",
            "Episode 380, Return: 24.00, Loss: 28.4717\n",
            "Episode 390, Return: 55.00, Loss: 95.6148\n",
            "Episode 400, Return: 37.00, Loss: 39.3406\n",
            "Episode 410, Return: 87.00, Loss: 263.1917\n",
            "Episode 420, Return: 41.00, Loss: 45.8112\n",
            "Episode 430, Return: 67.00, Loss: 137.9720\n",
            "Episode 440, Return: 67.00, Loss: 132.2803\n",
            "Episode 450, Return: 58.00, Loss: 89.8854\n",
            "Episode 460, Return: 48.00, Loss: 59.4646\n",
            "Episode 470, Return: 48.00, Loss: 54.1297\n",
            "Episode 480, Return: 47.00, Loss: 56.6282\n",
            "Episode 490, Return: 40.00, Loss: 44.8803\n",
            "Episode 500, Return: 57.00, Loss: 78.1260\n",
            "Episode 510, Return: 49.00, Loss: 56.4177\n",
            "Episode 520, Return: 55.00, Loss: 71.0813\n",
            "Episode 530, Return: 75.00, Loss: 135.6376\n",
            "Episode 540, Return: 52.00, Loss: 60.1389\n",
            "Episode 550, Return: 42.00, Loss: 55.0865\n",
            "Episode 560, Return: 56.00, Loss: 72.0625\n",
            "Episode 570, Return: 64.00, Loss: 91.5596\n",
            "Episode 580, Return: 52.00, Loss: 52.7626\n",
            "Episode 590, Return: 36.00, Loss: 58.2306\n",
            "Episode 600, Return: 37.00, Loss: 52.6457\n",
            "Episode 610, Return: 77.00, Loss: 119.3004\n",
            "Episode 620, Return: 93.00, Loss: 190.9808\n",
            "Episode 630, Return: 60.00, Loss: 66.8660\n",
            "Episode 640, Return: 75.00, Loss: 116.5404\n",
            "Episode 650, Return: 33.00, Loss: 96.0427\n",
            "Episode 660, Return: 68.00, Loss: 72.3642\n",
            "Episode 670, Return: 62.00, Loss: 62.3897\n",
            "Episode 680, Return: 65.00, Loss: 66.1564\n",
            "Episode 690, Return: 107.00, Loss: 198.1775\n",
            "Episode 700, Return: 66.00, Loss: 97.8408\n",
            "Episode 710, Return: 72.00, Loss: 64.9861\n",
            "Episode 720, Return: 59.00, Loss: 78.6036\n",
            "Episode 730, Return: 43.00, Loss: 84.8782\n",
            "Episode 740, Return: 59.00, Loss: 68.6007\n",
            "Episode 750, Return: 74.00, Loss: 102.8399\n",
            "Episode 760, Return: 86.00, Loss: 107.6476\n",
            "Episode 770, Return: 63.00, Loss: 88.7693\n",
            "Episode 780, Return: 134.00, Loss: 306.8201\n",
            "Episode 790, Return: 110.00, Loss: 222.7842\n",
            "Episode 800, Return: 175.00, Loss: 433.2292\n",
            "Episode 810, Return: 122.00, Loss: 186.6119\n",
            "Episode 820, Return: 113.00, Loss: 180.9039\n",
            "Episode 830, Return: 117.00, Loss: 141.4325\n",
            "Episode 840, Return: 168.00, Loss: 299.1825\n",
            "Episode 850, Return: 144.00, Loss: 258.9933\n",
            "Episode 860, Return: 196.00, Loss: 337.7857\n",
            "Episode 870, Return: 90.00, Loss: 166.0231\n",
            "Episode 880, Return: 125.00, Loss: 184.6019\n",
            "Episode 890, Return: 112.00, Loss: 160.3629\n",
            "Episode 900, Return: 48.00, Loss: 282.6690\n",
            "Episode 910, Return: 104.00, Loss: 66.9367\n",
            "Episode 920, Return: 113.00, Loss: 94.1659\n",
            "Episode 930, Return: 154.00, Loss: 155.2960\n",
            "Episode 940, Return: 83.00, Loss: 50.9334\n",
            "Episode 950, Return: 156.00, Loss: 150.3132\n",
            "Episode 960, Return: 102.00, Loss: 35.7679\n",
            "Episode 970, Return: 114.00, Loss: 53.4477\n",
            "Episode 980, Return: 165.00, Loss: 264.7817\n",
            "Episode 990, Return: 122.00, Loss: 103.4511\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "import time"
      ],
      "metadata": {
        "id": "9mMVBEkXA_hb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state = env.reset()\n",
        "total_reward = 0\n",
        "done=False\n",
        "\n",
        "while not done:\n",
        "  frame = env.render()\n",
        "  frame=frame[0]\n",
        "\n",
        "  plt.imshow(frame)\n",
        "  plt.axis('off')\n",
        "  display(plt.gcf())\n",
        "  clear_output(wait=True)\n",
        "  time.sleep(0.03)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "    logits, _ = model(state_tensor)\n",
        "    probs = torch.softmax(logits, dim=-1)\n",
        "    dist = torch.distributions.Categorical(probs)\n",
        "    action = dist.sample().item()\n",
        "\n",
        "  next_state, reward, done, truncated = env.step(action)\n",
        "  total_reward += reward\n",
        "  state = next_state\n",
        "env.close()\n",
        "print(f\"Total Reward: {total_reward}\")"
      ],
      "metadata": {
        "id": "xV4TeZBSAxxU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}